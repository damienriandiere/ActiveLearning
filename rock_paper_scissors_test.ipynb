{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from datasetB_main import MyDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import os\n",
    "import datetime\n",
    "import platform\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Keras version:', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists('./tmp'):\n",
    "\tshutil.rmtree('./tmp')\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "\n",
    "builder = MyDataset(data_dir='tmp')\n",
    "builder.download_and_prepare(\n",
    "\tdownload_dir='tmp',\n",
    "\tdownload_config=tfds.download.DownloadConfig(manual_dir='tmp')\n",
    ")\n",
    "dataset_train_raw = builder.as_dataset(split='train')\n",
    "dataset_test_raw = builder.as_dataset(split='test')\n",
    "dataset_val_raw = builder.as_dataset(split='val')\n",
    "dataset_info = builder.info\n",
    "\n",
    "print(dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Raw train dataset:', dataset_train_raw)\n",
    "print('Raw train dataset size:', len(dataset_train_raw), '\\n')\n",
    "\n",
    "print('Raw test dataset:', dataset_test_raw)\n",
    "print('Raw test dataset size:', len(dataset_test_raw), '\\n')\n",
    "\n",
    "print('Raw val dataset:', dataset_val_raw)\n",
    "print('Raw val dataset size:', len(dataset_val_raw), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_EXAMPLES = dataset_info.splits['train'].num_examples\n",
    "NUM_TEST_EXAMPLES = dataset_info.splits['test'].num_examples\n",
    "NUM_VAL_EXAMPLES = dataset_info.splits['val'].num_examples\n",
    "NUM_CLASSES = dataset_info.features['label'].num_classes\n",
    "\n",
    "print('Number of TRAIN examples:', NUM_TRAIN_EXAMPLES)\n",
    "print('Number of TEST examples:', NUM_TEST_EXAMPLES)\n",
    "print('Number of VAL examples:', NUM_VAL_EXAMPLES)\n",
    "print('Number of label classes:', NUM_CLASSES)\n",
    "label_names = dataset_info.features['label'].names\n",
    "print(f\"Labels (classes) : {label_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMG_SIZE_ORIGINAL = dataset_info.features['image'].shape[0]\n",
    "INPUT_IMG_SHAPE_ORIGINAL = dataset_info.features['image'].shape\n",
    "\n",
    "INPUT_IMG_SIZE_REDUCED = INPUT_IMG_SIZE_ORIGINAL // 2\n",
    "INPUT_IMG_SHAPE_REDUCED = (\n",
    "    INPUT_IMG_SIZE_REDUCED,\n",
    "    INPUT_IMG_SIZE_REDUCED,\n",
    "    INPUT_IMG_SHAPE_ORIGINAL[2]\n",
    ")\n",
    "\n",
    "# Here we may switch between bigger or smaller image sized that we will train our model on.\n",
    "INPUT_IMG_SIZE = INPUT_IMG_SIZE_REDUCED\n",
    "INPUT_IMG_SHAPE = INPUT_IMG_SHAPE_REDUCED\n",
    "\n",
    "print('Input image size (original):', INPUT_IMG_SIZE_ORIGINAL)\n",
    "print('Input image shape (original):', INPUT_IMG_SHAPE_ORIGINAL)\n",
    "print('\\n')\n",
    "print('Input image size (reduced):', INPUT_IMG_SIZE_REDUCED)\n",
    "print('Input image shape (reduced):', INPUT_IMG_SHAPE_REDUCED)\n",
    "print('\\n')\n",
    "print('Input image size:', INPUT_IMG_SIZE)\n",
    "print('Input image shape:', INPUT_IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert label ID to labels string.\n",
    "get_label_name = dataset_info.features['label'].int2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_label_name(0))\n",
    "print(get_label_name(1))\n",
    "print(get_label_name(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_dataset(dataset, size=12):\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plot_index = 0\n",
    "    nb_rocks = 0\n",
    "    nb_scissors = 0\n",
    "    nb_paper = 0\n",
    "    for image, label in dataset.take(size):\n",
    "        plot_index += 1\n",
    "        if get_label_name(label.numpy()) == 'rock':\n",
    "            nb_rocks += 1\n",
    "        elif get_label_name(label.numpy()) == 'scissors':\n",
    "            nb_scissors += 1\n",
    "        elif get_label_name(label.numpy()) == 'paper':\n",
    "            nb_paper += 1\n",
    "        plt.subplot(size//3, size//3, plot_index)\n",
    "        # plt.axis('Off')\n",
    "        plt.title('Label: %s' % get_label_name(label.numpy()))\n",
    "        plt.imshow(image.numpy())\n",
    "    print(f'rocks: {nb_rocks}, scissors: {nb_scissors}, paper: {nb_paper}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore raw training dataset images.\n",
    "#preview_dataset(dataset_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(features):\n",
    "    image = features['image']\n",
    "    label = features['label']\n",
    "    # Make image color values to be float.\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Make image color values to be in [0..1] range.\n",
    "    image = image / 255.\n",
    "    # Make sure that image has a right size\n",
    "    image = tf.image.resize(image, [INPUT_IMG_SIZE, INPUT_IMG_SIZE])\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train_raw)\n",
    "dataset_train = dataset_train_raw.map(format_example)\n",
    "dataset_test = dataset_test_raw.map(format_example)\n",
    "dataset_val = dataset_val_raw.map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset_train = dataset_train.batch(BATCH_SIZE)\n",
    "\n",
    "DATASET_TEST = dataset_test.batch(BATCH_SIZE)\n",
    "\n",
    "DATASET_VAL = dataset_val.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model = tf.keras.models.load_model('rock_paper_scissors_cnn.keras')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model with our dataset without fit\n",
    "model.evaluate(DATASET_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budgets = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing callbacks.\n",
    "os.makedirs('logs/fit', exist_ok=True)\n",
    "tensorboard_log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=tensorboard_log_dir,\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "os.makedirs('tmp/checkpoints', exist_ok=True)\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='tmp/checkpoints/weights.{epoch:02d}-{val_loss:.2f}.keras'\n",
    ")\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    monitor='val_accuracy'\n",
    "    # monitor='val_loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def selectRandomSamples(dataset_train, num_samples):\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for image_batch, label_batch in dataset_train:\n",
    "        images.extend(image_batch.numpy())  \n",
    "        labels.extend(label_batch.numpy())\n",
    "\n",
    "    random_indices = [random.randint(0, NUM_TRAIN_EXAMPLES - 1) for _ in range(num_samples)]\n",
    "\n",
    "    selected_images = [images[idx] for idx in random_indices]\n",
    "    selected_labels = [labels[idx] for idx in random_indices]\n",
    "\n",
    "    dataset_train_random = tf.data.Dataset.from_tensor_slices((selected_images, selected_labels))\n",
    "    dataset_train_random = dataset_train_random.shuffle(buffer_size=num_samples)\n",
    "    dataset_train_random = dataset_train_random.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset_train_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_confidence(model, dataset_train, num_samples):\n",
    "    \"\"\"\n",
    "    Sélectionne les échantillons les moins confiants pour l'entraînement basé sur un modèle de deep learning.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Modèle TensorFlow entraîné pour effectuer des prédictions.\n",
    "        dataset_train (tf.data.Dataset): Dataset TensorFlow contenant les données d'entraînement (images, labels).\n",
    "        num_samples (int): Nombre d'échantillons à sélectionner (parmi l'ensemble d'entraînement), \n",
    "                           représentant le pourcentage de données les moins confiantes à utiliser.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Un nouveau dataset TensorFlow contenant les échantillons sélectionnés (les moins confiants).\n",
    "    \"\"\"\n",
    "    # Liste pour stocker les incertitudes et les indices\n",
    "    uncertainties = []\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Parcourir le dataset en batch\n",
    "    for image_batch, label_batch in dataset_train:\n",
    "        preds = model(image_batch, training=False)  # Prédictions sur le batch\n",
    "        batch_uncertainties = 1 - np.max(preds.numpy(), axis=1)  # Calculer l'incertitude pour chaque image\n",
    "        uncertainties.extend(batch_uncertainties)\n",
    "        images.extend(image_batch.numpy())  # Ajouter les images du batch\n",
    "        labels.extend(label_batch.numpy())  # Ajouter les labels du batch\n",
    "\n",
    "    # Trier les incertitudes et récupérer les indices des plus incertains\n",
    "    sorted_indices = np.argsort(uncertainties)[:num_samples]\n",
    "\n",
    "    # Sélectionner les images et labels correspondants aux indices les moins confiants\n",
    "    selected_images = [images[idx] for idx in sorted_indices]\n",
    "    selected_labels = [labels[idx] for idx in sorted_indices]\n",
    "\n",
    "    # Créer un nouveau dataset TensorFlow à partir des images et labels sélectionnés\n",
    "    dataset_train_least_confidence = tf.data.Dataset.from_tensor_slices((selected_images, selected_labels))\n",
    "    dataset_train_least_confidence = dataset_train_least_confidence.shuffle(buffer_size=num_samples)\n",
    "    dataset_train_least_confidence = dataset_train_least_confidence.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset_train_least_confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def entropy_based_sampling(model, dataset_train, num_samples):\n",
    "    \"\"\"\n",
    "    Sélectionne les échantillons avec la plus grande entropie pour l'entraînement basé sur un modèle de deep learning.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Modèle TensorFlow entraîné pour effectuer des prédictions.\n",
    "        dataset_train (tf.data.Dataset): Dataset TensorFlow contenant les données d'entraînement (images, labels).\n",
    "        num_samples (int): Nombre d'échantillons à sélectionner (parmi l'ensemble d'entraînement), \n",
    "                           représentant les données les plus incertaines à utiliser.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Un nouveau dataset TensorFlow contenant les échantillons sélectionnés (avec la plus grande entropie).\n",
    "    \"\"\"\n",
    "    # Liste pour stocker les entropies et les indices\n",
    "    entropies = []\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Parcourir le dataset en batch\n",
    "    for image_batch, label_batch in dataset_train:\n",
    "        preds = model(image_batch, training=False)  # Prédictions sur le batch\n",
    "        \n",
    "        # Appliquer la fonction d'entropie de SciPy pour chaque batch\n",
    "        batch_entropies = entropy(preds.numpy().T)  # Entropie de Shannon pour chaque image\n",
    "        entropies.extend(batch_entropies)\n",
    "        images.extend(image_batch.numpy())  # Ajouter les images du batch\n",
    "        labels.extend(label_batch.numpy())  # Ajouter les labels du batch\n",
    "\n",
    "    # Trier les entropies et récupérer les indices des plus incertains (avec la plus grande entropie)\n",
    "    sorted_indices = np.argsort(entropies)[-num_samples:]\n",
    "\n",
    "    # Sélectionner les images et labels correspondants aux indices les plus incertains\n",
    "    selected_images = [images[idx] for idx in sorted_indices]\n",
    "    selected_labels = [labels[idx] for idx in sorted_indices]\n",
    "\n",
    "    # Créer un nouveau dataset TensorFlow à partir des images et labels sélectionnés\n",
    "    dataset_train_entropy_based = tf.data.Dataset.from_tensor_slices((selected_images, selected_labels))\n",
    "    dataset_train_entropy_based = dataset_train_entropy_based.shuffle(buffer_size=num_samples)\n",
    "    dataset_train_entropy_based = dataset_train_entropy_based.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset_train_entropy_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_based_sampling(model, dataset_train, num_samples):\n",
    "    \"\"\"\n",
    "    Sélectionne les échantillons avec la plus petite marge pour l'entraînement basé sur un modèle de deep learning.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Modèle TensorFlow entraîné pour effectuer des prédictions.\n",
    "        dataset_train (tf.data.Dataset): Dataset TensorFlow contenant les données d'entraînement (images, labels).\n",
    "        num_samples (int): Nombre d'échantillons à sélectionner (parmi l'ensemble d'entraînement),\n",
    "                           représentant les données avec la plus petite marge à utiliser.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Un nouveau dataset TensorFlow contenant les échantillons sélectionnés (avec la plus petite marge).\n",
    "    \"\"\"\n",
    "    # Liste pour stocker les marges et les indices\n",
    "    margins = []\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Parcourir le dataset en batch\n",
    "    for image_batch, label_batch in dataset_train:\n",
    "        preds = model(image_batch, training=False)  # Prédictions sur le batch\n",
    "        \n",
    "        # Calculer la marge : différence entre les deux plus grandes probabilités\n",
    "        top2_probs = np.partition(preds.numpy(), -2)[:, -2:]  # Top 2 plus grandes valeurs de probabilité\n",
    "        batch_margins = top2_probs[:, 1] - top2_probs[:, 0]  # Calcul de la marge (différence entre les deux plus grandes)\n",
    "        \n",
    "        margins.extend(batch_margins)\n",
    "        images.extend(image_batch.numpy())  # Ajouter les images du batch\n",
    "        labels.extend(label_batch.numpy())  # Ajouter les labels du batch\n",
    "\n",
    "    # Trier les marges et récupérer les indices des plus petites marges\n",
    "    sorted_indices = np.argsort(margins)[:num_samples]\n",
    "\n",
    "    # Sélectionner les images et labels correspondants aux indices des plus petites marges\n",
    "    selected_images = [images[idx] for idx in sorted_indices]\n",
    "    selected_labels = [labels[idx] for idx in sorted_indices]\n",
    "\n",
    "    # Créer un nouveau dataset TensorFlow à partir des images et labels sélectionnés\n",
    "    dataset_train_margin_based = tf.data.Dataset.from_tensor_slices((selected_images, selected_labels))\n",
    "    preview_dataset(dataset_train_margin_based, num_samples)\n",
    "    dataset_train_margin_based = dataset_train_margin_based.shuffle(buffer_size=num_samples)\n",
    "    dataset_train_margin_based = dataset_train_margin_based.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset_train_margin_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_sampling(model, dataset_train, num_samples):\n",
    "    \"\"\"\n",
    "    Sélectionne les échantillons avec le plus faible ratio entre la probabilité de la classe la plus élevée\n",
    "    et la somme des autres probabilités, pour l'entraînement basé sur un modèle de deep learning.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Modèle TensorFlow entraîné pour effectuer des prédictions.\n",
    "        dataset_train (tf.data.Dataset): Dataset TensorFlow contenant les données d'entraînement (images, labels).\n",
    "        num_samples (int): Nombre d'échantillons à sélectionner (parmi l'ensemble d'entraînement),\n",
    "                           représentant les données avec le ratio le plus faible à utiliser.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Un nouveau dataset TensorFlow contenant les échantillons sélectionnés (avec le plus faible ratio).\n",
    "    \"\"\"\n",
    "    # Liste pour stocker les ratios et les indices\n",
    "    ratios = []\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Parcourir le dataset en batch\n",
    "    for image_batch, label_batch in dataset_train:\n",
    "        preds = model(image_batch, training=False)  # Prédictions sur le batch\n",
    "        \n",
    "        # Calculer le ratio : probabilité de la classe la plus probable / somme des autres probabilités\n",
    "        top_prob = np.max(preds.numpy(), axis=1)  # Probabilité de la classe la plus probable\n",
    "        sum_probs = np.sum(preds.numpy(), axis=1) - top_prob  # Somme des autres probabilités\n",
    "        batch_ratios = top_prob / (sum_probs + 1e-10)  # Calcul du ratio (ajout d'un petit epsilon pour éviter la division par zéro)\n",
    "        \n",
    "        ratios.extend(batch_ratios)\n",
    "        images.extend(image_batch.numpy())  # Ajouter les images du batch\n",
    "        labels.extend(label_batch.numpy())  # Ajouter les labels du batch\n",
    "\n",
    "    # Trier les ratios et récupérer les indices des plus faibles ratios\n",
    "    sorted_indices = np.argsort(ratios)[:num_samples]\n",
    "\n",
    "    # Sélectionner les images et labels correspondants aux indices des plus faibles ratios\n",
    "    selected_images = [images[idx] for idx in sorted_indices]\n",
    "    selected_labels = [labels[idx] for idx in sorted_indices]\n",
    "\n",
    "    # Créer un nouveau dataset TensorFlow à partir des images et labels sélectionnés\n",
    "    dataset_train_ratio_based = tf.data.Dataset.from_tensor_slices((selected_images, selected_labels))\n",
    "    dataset_train_ratio_based = dataset_train_ratio_based.shuffle(buffer_size=num_samples)\n",
    "    dataset_train_ratio_based = dataset_train_ratio_based.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset_train_ratio_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm as euclidean_dis\n",
    "\n",
    "def model_based_outlier_sampling(model, dataset_train, num_samples):\n",
    "    \"\"\"\n",
    "    Sélectionne les échantillons les plus éloignés des prédictions moyennes du modèle (outliers).\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Modèle TensorFlow entraîné pour effectuer des prédictions.\n",
    "        dataset_train (tf.data.Dataset): Dataset TensorFlow contenant les données d'entraînement (images, labels).\n",
    "        num_samples (int): Nombre d'échantillons à sélectionner (parmi l'ensemble d'entraînement),\n",
    "                           représentant les outliers prédits par le modèle.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Un nouveau dataset TensorFlow contenant les échantillons sélectionnés (outliers).\n",
    "    \"\"\"\n",
    "    # Liste pour stocker les distances aux prédictions moyennes\n",
    "    distances = []\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Parcourir le dataset en batch pour collecter toutes les prédictions\n",
    "    all_preds = []\n",
    "    for image_batch, _ in dataset_train:\n",
    "        preds = model(image_batch, training=False)  # Prédictions sur le batch\n",
    "        all_preds.append(preds.numpy())\n",
    "    all_preds = np.vstack(all_preds)  # Convertir en une seule matrice\n",
    "    mean_pred = np.mean(all_preds, axis=0)  # Moyenne des prédictions globales\n",
    "\n",
    "    # Calculer la distance entre chaque prédiction et la prédiction moyenne\n",
    "    for image_batch, label_batch in dataset_train:\n",
    "        preds = model(image_batch, training=False)  # Prédictions sur le batch\n",
    "        batch_distances = euclidean_dis(preds.numpy() - mean_pred, axis=1)  # Distance euclidienne\n",
    "        distances.extend(batch_distances)\n",
    "        images.extend(image_batch.numpy())  # Ajouter les images du batch\n",
    "        labels.extend(label_batch.numpy())  # Ajouter les labels du batch\n",
    "\n",
    "    # Trier les distances et récupérer les indices des outliers (distances les plus grandes)\n",
    "    sorted_indices = np.argsort(distances)[-num_samples:]\n",
    "\n",
    "    # Sélectionner les images et labels correspondants aux indices des outliers\n",
    "    selected_images = [images[idx] for idx in sorted_indices]\n",
    "    selected_labels = [labels[idx] for idx in sorted_indices]\n",
    "\n",
    "    # Créer un nouveau dataset TensorFlow à partir des images et labels sélectionnés\n",
    "    dataset_train_outliers = tf.data.Dataset.from_tensor_slices((selected_images, selected_labels))\n",
    "    dataset_train_outliers = dataset_train_outliers.shuffle(buffer_size=num_samples)\n",
    "    dataset_train_outliers = dataset_train_outliers.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset_train_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_based_sampling(model, dataset_train, num_samples, num_clusters=10):\n",
    "    \"\"\"\n",
    "    Sélectionne les échantillons basés sur le clustering pour l'entraînement actif.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Modèle TensorFlow préalablement entraîné pour extraire des représentations.\n",
    "        dataset_train (tf.data.Dataset): Dataset TensorFlow contenant les données d'entraînement (images, labels).\n",
    "        num_samples (int): Nombre d'échantillons à sélectionner (parmi l'ensemble d'entraînement).\n",
    "        num_clusters (int): Nombre de clusters à former dans l'espace des représentations.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Un nouveau dataset TensorFlow contenant les échantillons sélectionnés.\n",
    "    \"\"\"\n",
    "    # Listes pour stocker les représentations, les images et les labels\n",
    "    embeddings = []\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Parcourir le dataset pour extraire les représentations\n",
    "    for image_batch, label_batch in dataset_train:\n",
    "        batch_embeddings = model(image_batch, training=False).numpy()  # Extraire les représentations\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        images.extend(image_batch.numpy())  # Ajouter les images du batch\n",
    "        labels.extend(label_batch.numpy())  # Ajouter les labels du batch\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    # Appliquer l'algorithme de clustering KMeans\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # Sélectionner les échantillons les plus incertains ou les plus représentatifs de chaque cluster\n",
    "    selected_indices = []\n",
    "    for cluster in range(num_clusters):\n",
    "        # Trouver les indices des points appartenant au cluster\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "\n",
    "        # Si le cluster contient des points, choisir un échantillon central\n",
    "        if len(cluster_indices) > 0:\n",
    "            # Calculer les distances par rapport au centroïde\n",
    "            cluster_center = kmeans.cluster_centers_[cluster]\n",
    "            distances = np.linalg.norm(embeddings[cluster_indices] - cluster_center, axis=1)\n",
    "\n",
    "            # Sélectionner l'index du point le plus proche du centroïde (point incertain)\n",
    "            selected_index = cluster_indices[np.argmin(distances)]\n",
    "            selected_indices.append(selected_index)\n",
    "\n",
    "    # Sélectionner les images et labels correspondants\n",
    "    selected_images = [images[idx] for idx in selected_indices]\n",
    "    selected_labels = [labels[idx] for idx in selected_indices]\n",
    "\n",
    "    # Créer un nouveau dataset TensorFlow à partir des images et labels sélectionnés\n",
    "    dataset_train_cluster_based = tf.data.Dataset.from_tensor_slices((selected_images, selected_labels))\n",
    "    dataset_train_cluster_based = dataset_train_cluster_based.shuffle(buffer_size=num_samples)\n",
    "    dataset_train_cluster_based = dataset_train_cluster_based.batch(BATCH_SIZE)  # Définir la taille de batch\n",
    "\n",
    "    return dataset_train_cluster_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def representative_sampling(model, dataset_train, num_samples):\n",
    "    \"\"\"\n",
    "    Sélectionne les échantillons les plus représentatifs pour l'entraînement basé sur un modèle de deep learning.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Modèle TensorFlow entraîné pour extraire des représentations.\n",
    "        dataset_train (tf.data.Dataset): Dataset TensorFlow contenant les données d'entraînement (images, labels).\n",
    "        num_samples (int): Nombre d'échantillons à sélectionner (parmi l'ensemble d'entraînement), \n",
    "                           représentant les données les plus représentatives.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Un nouveau dataset TensorFlow contenant les échantillons sélectionnés (les plus représentatifs).\n",
    "    \"\"\"\n",
    "    # Listes pour stocker les représentations, les images et les labels\n",
    "    embeddings = []\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Parcourir le dataset pour extraire les représentations\n",
    "    for image_batch, label_batch in dataset_train:\n",
    "        batch_embeddings = model(image_batch, training=False).numpy()  # Extraire les représentations\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        images.extend(image_batch.numpy())  # Ajouter les images du batch\n",
    "        labels.extend(label_batch.numpy())  # Ajouter les labels du batch\n",
    "\n",
    "    # Calculer la distance moyenne de chaque point à tous les autres\n",
    "    distance_matrix = euclidean_distances(embeddings, embeddings)\n",
    "    mean_distances = np.mean(distance_matrix, axis=1)\n",
    "\n",
    "    # Trier les distances moyennes et récupérer les indices des plus petites (les plus représentatives)\n",
    "    sorted_indices = np.argsort(mean_distances)[:num_samples]\n",
    "\n",
    "    # Sélectionner les images et labels correspondants\n",
    "    selected_images = [images[idx] for idx in sorted_indices]\n",
    "    selected_labels = [labels[idx] for idx in sorted_indices]\n",
    "\n",
    "    # Créer un nouveau dataset TensorFlow à partir des images et labels sélectionnés\n",
    "    dataset_train_representative = tf.data.Dataset.from_tensor_slices((selected_images, selected_labels))\n",
    "    preview_dataset(dataset_train_representative, num_samples)\n",
    "    dataset_train_representative = dataset_train_representative.shuffle(buffer_size=num_samples)\n",
    "    dataset_train_representative = dataset_train_representative.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset_train_representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_sampling_sequential(model, dataset_train, num_samples, rep_samples_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Combine Representative Sampling et Margin of Confidence via une combinaison séquentielle.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Modèle TensorFlow entraîné pour effectuer des prédictions.\n",
    "        dataset_train (tf.data.Dataset): Dataset TensorFlow contenant les données d'entraînement (images, labels).\n",
    "        num_samples (int): Nombre total d'échantillons à sélectionner.\n",
    "        rep_samples_ratio (float): Proportion d'échantillons pour Representative Sampling avant d'appliquer Margin of Confidence.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Un nouveau dataset TensorFlow contenant les échantillons combinés.\n",
    "    \"\"\"\n",
    "    # Étape 1 : Calcul du nombre d'échantillons pour Representative Sampling\n",
    "    rep_samples = int(NUM_TRAIN_EXAMPLES * rep_samples_ratio)\n",
    "    print(\"Échantillons pour Representative Sampling:\", rep_samples)\n",
    "\n",
    "    # Étape 2 : Appliquer Representative Sampling\n",
    "    print(\"Étape 1: Representative Sampling\")\n",
    "    rep_dataset = representative_sampling(model, dataset_train, rep_samples)\n",
    "\n",
    "    # Étape 3 : Appliquer Margin of Confidence sur le sous-ensemble réduit\n",
    "    print(\"Étape 2: Margin of Confidence\")\n",
    "    final_dataset = margin_based_sampling(model, rep_dataset, num_samples)\n",
    "\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_based_sampling_inversed(model, dataset_train, num_samples):\n",
    "    \"\"\"\n",
    "    Sélectionne les échantillons avec la plus grande marge pour l'entraînement basé sur un modèle de deep learning.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Modèle TensorFlow entraîné pour effectuer des prédictions.\n",
    "        dataset_train (tf.data.Dataset): Dataset TensorFlow contenant les données d'entraînement (images, labels).\n",
    "        num_samples (int): Nombre d'échantillons à sélectionner (parmi l'ensemble d'entraînement),\n",
    "                           représentant les données avec la plus petite marge à utiliser.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Un nouveau dataset TensorFlow contenant les échantillons sélectionnés (avec la plus petite marge).\n",
    "    \"\"\"\n",
    "    # Liste pour stocker les marges et les indices\n",
    "    margins = []\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Parcourir le dataset en batch\n",
    "    for image_batch, label_batch in dataset_train:\n",
    "        preds = model(image_batch, training=False)  # Prédictions sur le batch\n",
    "        \n",
    "        # Calculer la marge : différence entre les deux plus grandes probabilités\n",
    "        top2_probs = np.partition(preds.numpy(), -2)[:, -2:]  # Top 2 plus grandes valeurs de probabilité\n",
    "        batch_margins = top2_probs[:, 1] - top2_probs[:, 0]  # Calcul de la marge (différence entre les deux plus grandes)\n",
    "        \n",
    "        margins.extend(batch_margins)\n",
    "        images.extend(image_batch.numpy())  # Ajouter les images du batch\n",
    "        labels.extend(label_batch.numpy())  # Ajouter les labels du batch\n",
    "\n",
    "    # Trier les marges et récupérer les indices des plus grandes marges\n",
    "    sorted_indices = np.argsort(margins)[-num_samples:]\n",
    "\n",
    "    # Sélectionner les images et labels correspondants aux indices des plus grandes marges\n",
    "    selected_images = [images[idx] for idx in sorted_indices]\n",
    "    selected_labels = [labels[idx] for idx in sorted_indices]\n",
    "\n",
    "    # Créer un nouveau dataset TensorFlow à partir des images et labels sélectionnés\n",
    "    dataset_train_margin_based = tf.data.Dataset.from_tensor_slices((selected_images, selected_labels))\n",
    "    dataset_train_margin_based = dataset_train_margin_based.shuffle(buffer_size=num_samples)\n",
    "    dataset_train_margin_based = dataset_train_margin_based.batch(BATCH_SIZE)\n",
    "\n",
    "    return dataset_train_margin_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(dataset_train, method, budget=0.0):\n",
    "    model = tf.keras.models.load_model('rock_paper_scissors_cnn.keras')\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=1,\n",
    "    monitor='val_accuracy'                                                                                              \n",
    "    )\n",
    "\n",
    "    num_samples = int(budget * NUM_TRAIN_EXAMPLES)\n",
    "    print(f'Number of training examples to select: {num_samples}')\n",
    "    \n",
    "    if method == 'RANDOM':\n",
    "        dataset_to_train = selectRandomSamples(dataset_train, num_samples)\n",
    "    elif method == 'LEAST CONFIDENCE':\n",
    "        dataset_to_train = least_confidence(model, dataset_train, num_samples)\n",
    "    elif method == 'ENTROPY BASED':\n",
    "        dataset_to_train = entropy_based_sampling(model, dataset_train, num_samples)\n",
    "    elif method == 'RATIO SAMPLING':\n",
    "        dataset_to_train = ratio_sampling(model, dataset_train, num_samples)\n",
    "    elif method == 'MARGIN OF CONFIDENCE':\n",
    "        dataset_to_train = margin_based_sampling(model, dataset_train, num_samples)\n",
    "    elif method == 'MODEL BASED OUTLIER':\n",
    "        dataset_to_train = model_based_outlier_sampling(model, dataset_train, num_samples)\n",
    "    elif method == 'CLUSTER BASED':\n",
    "        dataset_to_train = cluster_based_sampling(model, dataset_train, num_samples)\n",
    "    elif method == 'REPRESENTATIVE SAMPLING':\n",
    "        dataset_to_train = representative_sampling(model, dataset_train, num_samples)\n",
    "    elif method == 'COMBINED SEQUENTIAL':\n",
    "        dataset_to_train = combined_sampling_sequential(model, dataset_train, num_samples)\n",
    "    elif method == 'MARGIN OF CONFIDENCE INVERSED':\n",
    "        dataset_to_train = margin_based_sampling_inversed(model, dataset_train, num_samples)\n",
    "        \n",
    "    model.fit(x=dataset_to_train.repeat(),\n",
    "    validation_data=DATASET_VAL.repeat(), epochs=50,\n",
    "    steps_per_epoch=num_samples // BATCH_SIZE,\n",
    "    validation_steps=NUM_TEST_EXAMPLES // BATCH_SIZE,\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback], verbose=0)\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(DATASET_TEST, verbose=0)\n",
    "    print(f\"Test Accuracy: {test_accuracy}, Test Loss: {test_loss}\")\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracyStats(dataset_train, method, budget=0.0, repeats=1):\n",
    "    accuracies = []\n",
    "    for i in range(repeats):\n",
    "        accuracies.append(retrain(dataset_train, method, budget))\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    if method == 'RANDOM':\n",
    "        accuracies = np.array(accuracies)\n",
    "        std_dev = np.std(accuracies)\n",
    "        confidence_interval = 1.96 * (std_dev / np.sqrt(repeats))\n",
    "        min_confidence = average_accuracy - confidence_interval\n",
    "        max_confidence = average_accuracy + confidence_interval\n",
    "        return average_accuracy, min_confidence, max_confidence\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in dataset_train.take(1):  # Prenez le premier batch\n",
    "    first_image = image[0].numpy()\n",
    "    first_label = label[0].numpy()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in dataset_train.take(1):  # Prenez le premier batch\n",
    "    second_image = image[0].numpy()\n",
    "    second_label = label[0].numpy()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(second_image, first_image)\n",
    "assert second_label == first_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_averages_accuracy = []\n",
    "random_min_confidence = []\n",
    "random_max_confidence = []\n",
    "nb_repeats = 5\n",
    "\n",
    "for budget in budgets:\n",
    "    average_accuracy, min_confidence, max_confidence = getAccuracyStats(dataset_train, 'RANDOM', budget, nb_repeats)\n",
    "    random_averages_accuracy.append(average_accuracy)\n",
    "    random_min_confidence.append(min_confidence)\n",
    "    random_max_confidence.append(max_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNCERTAINTY METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method n°1 : LEAST CONFIDENCE METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_confidence_accuracy = []\n",
    "for budget in budgets:\n",
    "    accuracy = getAccuracyStats(dataset_train, 'LEAST CONFIDENCE', budget)\n",
    "    least_confidence_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method n°2 : ENTROPY BASED METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_based_accuracy = []\n",
    "for budget in budgets:\n",
    "    accuracy = getAccuracyStats(dataset_train, 'ENTROPY BASED', budget)\n",
    "    entropy_based_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method n°3 : MARGIN OF CONFIDENCE METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_of_confidence_accuracy = []\n",
    "for budget in budgets:\n",
    "    accuracy = getAccuracyStats(dataset_train, 'MARGIN OF CONFIDENCE', budget)\n",
    "    margin_of_confidence_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method n°4 : Ratio Sampling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_sampling_accuracy = []\n",
    "for budget in budgets:\n",
    "    accuracy = getAccuracyStats(dataset_train, 'RATIO SAMPLING', budget)\n",
    "    ratio_sampling_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIVERSITY METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method n°1 : Model based outlier sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_based_outlier_sampling_accuracy = []\n",
    "for budget in budgets:\n",
    "    accuracy = getAccuracyStats(dataset_train, 'MODEL BASED OUTLIER', budget)\n",
    "    model_based_outlier_sampling_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method n°2 : Cluster based sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_based_sampling_accuracy = []\n",
    "for budget in budgets:\n",
    "    accuracy = getAccuracyStats(dataset_train, 'CLUSTER BASED', budget)\n",
    "    cluster_based_sampling_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method n°3 : Representative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_sampling_accuracy = []\n",
    "#for budget in budgets:\n",
    "accuracy = getAccuracyStats(dataset_train, 'REPRESENTATIVE SAMPLING', 0.05)\n",
    "representative_sampling_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Learning (Combined method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined method n°1 : Representative sampling + Ratio sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_methods_accuracy = []\n",
    "for budget in budgets:\n",
    "    accuracy = getAccuracyStats(dataset_train, 'COMBINED SEQUENTIAL', budget)\n",
    "    combined_methods_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(budgets, least_confidence_accuracy, label='Least confidence accuracy')\n",
    "plt.plot(budgets, entropy_based_accuracy, label='Entropy based accuracy')\n",
    "plt.plot(budgets, margin_of_confidence_accuracy, label='Margin of confidence accuracy')\n",
    "plt.plot(budgets, ratio_sampling_accuracy, label='Ratio sampling accuracy')\n",
    "plt.plot(budgets, model_based_outlier_sampling_accuracy, label='Model based outlier sampling accuracy')\n",
    "plt.plot(budgets, cluster_based_sampling_accuracy, label='Cluster based accuracy')\n",
    "plt.plot(budgets, representative_sampling_accuracy, label='Representative sampling accuracy')\n",
    "plt.plot(budgets, combined_methods_accuracy, label='Advanced active learning accuracy')\n",
    "plt.plot(budgets, random_averages_accuracy, label='Random average accuracy')\n",
    "plt.vlines(budgets, random_min_confidence, random_max_confidence, color='gray', label='Intervalle de confiance à 95%')\n",
    "plt.xlabel('budget of training data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for different budgets of training data with different active learning methods')\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.savefig('active_learning_accuracy_different_active_learning_methods_.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_of_confidence_inversed_accuracy = []\n",
    "for budget in budgets:\n",
    "    accuracy = getAccuracyStats(dataset_train, 'MARGIN OF CONFIDENCE INVERSED', budget)\n",
    "    margin_of_confidence_inversed_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(budgets, margin_of_confidence_accuracy, label='Margin of confidence accuracy')\n",
    "plt.plot(budgets, margin_of_confidence_inversed_accuracy, label='Margin of confidence inversed accuracy')\n",
    "plt.plot(budgets, random_averages_accuracy, label='Random average accuracy')\n",
    "plt.vlines(budgets, random_min_confidence, random_max_confidence, color='gray', label='Intervalle de confiance à 95%')\n",
    "plt.xlabel('budget of training data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for different budgets of training data with different active learning methods')\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.savefig('active_learning_accuracy_different_active_learning_methods_after_hypothesis.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
